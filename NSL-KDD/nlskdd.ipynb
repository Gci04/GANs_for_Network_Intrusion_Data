{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Vanilla Generative Adversarial Network for NLS-KDD <center/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import preprocessing\n",
    "from classifiers import *\n",
    "from utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.stats import norm\n",
    "from tensorflow.keras.losses import kullback_leibler_divergence\n",
    "# K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data & Standard Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test, label_mapping = preprocessing.get_data(encoding=\"Label\")\n",
    "x_train,y_train = train.drop(\"label\",axis=1),train.label.values\n",
    "x_test , y_test =  test.drop(\"label\",axis=1),test.label.values\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "x_train = Scaler.fit_transform(x_train)\n",
    "x_test = Scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generator, Descriminator & Full Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(data_dim, min_num_neurones):\n",
    "    model = tf.keras.models.Sequential(name='Discriminator')\n",
    "    \n",
    "    model.add(Dense(min_num_neurones*2, activation='relu',input_dim = data_dim ))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(min_num_neurones, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"sgd\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# d = create_discriminator(3,3)\n",
    "# d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(data_dim, min_num_neurones,noise_dim):\n",
    "    \n",
    "    model = tf.keras.models.Sequential(name='Generator')\n",
    "    \n",
    "    model.add(Dense(min_num_neurones, activation='relu',input_dim = noise_dim ))\n",
    "    model.add(Dense(min_num_neurones*2, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(min_num_neurones*4, activation='tanh'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(data_dim))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"sgd\")\n",
    "    \n",
    "    return model\n",
    "# g = create_generator(3,3,3)\n",
    "# g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gan(discriminator, generator, z_dim):\n",
    "    discriminator.trainable=False\n",
    "    gan_input = Input(shape=(z_dim,))\n",
    "    #x = generator(gan_input)\n",
    "    #gan_output= discriminator(x)\n",
    "    \n",
    "    gan_output = discriminator(generator(gan_input))\n",
    "    gan= Model(inputs = gan_input, outputs = gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "    return gan\n",
    "# gan = create_gan(d,g,3)\n",
    "# gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define batch generation & GAN training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, batch_size=1):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray\n",
    "        The input data to sample a into batch\n",
    "    size : int (default = 1)\n",
    "        Batch size\n",
    "\n",
    "    Return Value: ndarray - random choice of samples from the input X of batch_size\n",
    "    \"\"\"\n",
    "    batch_ix = np.random.choice(len(X), batch_size, replace=False)\n",
    "    return X[batch_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(arguments,X):\n",
    "    \n",
    "    [rand_noise_dim, nb_steps, batch_size,D_epochs, G_epochs, min_num_neurones] = arguments\n",
    "    \n",
    "    data_dim = X.shape[1]\n",
    "    combined_loss, disc_loss_generated, disc_loss_real = [], [], []\n",
    "    \n",
    "    # Creating GAN\n",
    "    generator = create_generator(data_dim,min_num_neurones,rand_noise_dim)\n",
    "    discriminator = create_discriminator(data_dim,min_num_neurones)\n",
    "    adversarial_model = create_gan(discriminator, generator,rand_noise_dim)\n",
    "    \n",
    "    #Start training\n",
    "    for epoch in range(1,nb_steps + 1 ):\n",
    "        K.set_learning_phase(1)\n",
    "        \n",
    "        #Train Discriminator\n",
    "        discriminator.trainable=True\n",
    "        for i in range(D_epochs):\n",
    "            np.random.seed(i+epoch)\n",
    "        \n",
    "            noise = np.random.normal(0,1, size=(batch_size, rand_dim))\n",
    "            generated_samples = generator.predict(noise)\n",
    "            real_samples = get_batch(X,batch_size)\n",
    "            \n",
    "            d_l_r = discriminator.train_on_batch(real_samples, np.random.uniform(low=0.999, high=1.0, size=batch_size))\n",
    "            d_l_g = discriminator.train_on_batch(generated_samples, np.random.uniform(low=0.0, high=0.0001, size=batch_size))\n",
    "        \n",
    "        #Freeze Discriminator\n",
    "        print(\"D lr : \".format(discriminator.optimizer.get_config()))\n",
    "        discriminator.trainable = False\n",
    "        disc_loss_generated.append(d_l_g)\n",
    "        disc_loss_real.append(d_l_r)\n",
    "        \n",
    "        #Train Generator\n",
    "        for i in range(G_epochs):\n",
    "            np.random.seed(i+epoch)\n",
    "            \n",
    "            noise = np.random.normal(0,1, size = (batch_size, rand_dim))\n",
    "            loss = adversarial_model.train_on_batch(noise, np.random.uniform(low=0.999, high=1.0, size=batch_size))\n",
    "            print(\"GAN lr : \".format(discriminator.optimizer.get_config()))\n",
    "        combined_loss.append(loss)\n",
    "        \n",
    "        #Do checkpointing\n",
    "        if epoch % 10 == 0:\n",
    "            K.set_learning_phase(0)\n",
    "            test_size = len(X)\n",
    "\n",
    "            z = np.random.normal(3,2,size=(test_size, rand_dim))\n",
    "            g_z = generator.predict(z)\n",
    "            \n",
    "            '''\n",
    "            p = norm.pdf(X.T)\n",
    "            q = norm.pdf(g_z.T)\n",
    "\n",
    "            norm_p = p/p.sum(axis=1,keepdims=1)\n",
    "            norm_q = q/q.sum(axis=1,keepdims=1)\n",
    "\n",
    "            tf_kl = kullback_leibler_divergence(tf.convert_to_tensor(norm_p, np.float32), tf.convert_to_tensor(norm_q, np.float32))\n",
    "            with tf.Session() as sess:\n",
    "                print(\"Tensorflow kullback_leibler_divergence : {}\".format(round(sum(sess.run(tf_kl)))))\n",
    "\n",
    "            print(\"Ephoc : {} ,Loss on fake: {}, Loss on real : {}\".format(epoch,d_l_g, d_l_r))\n",
    "            '''\n",
    "            fake_pred = np.array(adversarial_model.predict(z)).ravel()\n",
    "            real_pred = np.array(discriminator.predict(X)).ravel()\n",
    "\n",
    "            modelAccuracy(fake_pred,real_pred)\n",
    "        \n",
    "    return dict({\"generator_model\":generator,\"discriminator_model\":discriminator,\\\n",
    "            \"combined_model\":adversarial_model,\"generator_loss\":combined_loss,\\\n",
    "            \"disc_loss_generated\":disc_loss_generated,\"disc_loss_real\": disc_loss_real})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Train samples and set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generative Adversarial Networks\n",
    "att_ind = np.where(train.label != label_mapping[\"normal\"])[0]\n",
    "\n",
    "x = x_train[att_ind]\n",
    "n_to_generate = 2000\n",
    "\n",
    "rand_dim = 32\n",
    "base_n_count = 100\n",
    "\n",
    "combined_ep = 100\n",
    "batch_size = 128 if len(x) > 128 else len(x)\n",
    "\n",
    "ep_d = 1\n",
    "ep_g = 2\n",
    "learning_rate = 0.0001#5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Discriminator accuracy on Fake : 0.5300699300699301, Real : 0.906634828586048\n",
      "Discriminator accuracy on Fake : 0.029046563192904655, Real : 0.9469895957700836\n",
      "Discriminator accuracy on Fake : 0.0005457956677468873, Real : 0.9595258400136449\n",
      "Discriminator accuracy on Fake : 1.7056114617090227e-05, Real : 0.9671158110182501\n",
      "Discriminator accuracy on Fake : 0.0, Real : 0.9674398771959748\n",
      "Discriminator accuracy on Fake : 5.116834385127068e-05, Real : 0.9673375405082723\n",
      "Discriminator accuracy on Fake : 0.0006993006993006993, Real : 0.9685314685314685\n",
      "Discriminator accuracy on Fake : 0.0008528057308545113, Real : 0.9743134913866621\n",
      "Discriminator accuracy on Fake : 0.00040934675081016544, Real : 0.9740747057820228\n",
      "Discriminator accuracy on Fake : 0.0, Real : 0.9714992324748423\n",
      "Discriminator accuracy on Fake : 8.528057308545113e-05, Real : 0.9649155722326455\n",
      "Discriminator accuracy on Fake : 5.116834385127068e-05, Real : 0.960685655807607\n",
      "Discriminator accuracy on Fake : 0.0007845812723861504, Real : 0.6531297970322361\n",
      "Discriminator accuracy on Fake : 0.03075217465461368, Real : 0.4034794473818864\n",
      "Discriminator accuracy on Fake : 0.35081016544431176, Real : 0.39397919154016714\n",
      "Discriminator accuracy on Fake : 0.988282449258059, Real : 0.3945420433225311\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.3958212519188129\n",
      "Discriminator accuracy on Fake : 0.9999658877707658, Real : 0.4930581613508443\n",
      "Discriminator accuracy on Fake : 0.9991983626129968, Real : 0.5504349309227358\n",
      "Discriminator accuracy on Fake : 0.9903974074705783, Real : 0.5014497697424527\n",
      "Discriminator accuracy on Fake : 0.9874637557564386, Real : 0.4148558758314856\n",
      "Discriminator accuracy on Fake : 0.9999658877707658, Real : 0.4073341292853488\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.4088521234862698\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.410216612655637\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.4115469895957701\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.4134402183182671\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.41342316220365\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.41333788163056456\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.4134402183182671\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.41355961112058676\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.41371311615214057\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.4178577520040935\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.4156745693331059\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.413986013986014\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.41403718232986525\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.4140712945590994\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.4140883506737165\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.4141224629029507\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.41425891181988744\n",
      "Discriminator accuracy on Fake : 0.9999829438853829, Real : 0.420416169196657\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.48400136448916936\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.5805389732219001\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.6286713286713287\n",
      "Discriminator accuracy on Fake : 0.9670134743305475, Real : 0.6107624083233839\n",
      "Discriminator accuracy on Fake : 0.9602421968275627, Real : 0.5552276991301381\n",
      "Discriminator accuracy on Fake : 0.9982261640798226, Real : 0.48128944226505205\n",
      "Discriminator accuracy on Fake : 1.0, Real : 0.41927340951731196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-17d072462282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrand_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_ep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mep_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_n_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-580784f6492c>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(arguments, X)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ephoc : {} ,Loss on fake: {}, Loss on real : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_l_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_l_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             '''\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mfake_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversarial_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mreal_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1113\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m-> 3079\u001b[0;31m                                  fetched[:len(self.outputs)])\n\u001b[0m\u001b[1;32m   3080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m   \"\"\"\n\u001b[0;32m--> 306\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flat_sequence must be a sequence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "arguments = [rand_dim, combined_ep, batch_size, ep_d,ep_g, base_n_count]\n",
    "res = training(arguments,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"combined_model\"].summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
